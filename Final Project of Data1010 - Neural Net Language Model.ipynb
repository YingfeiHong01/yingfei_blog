{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "prismiaId": "b55d9e27-9f59-476f-b53e-8ea225209889"
   },
   "source": [
    "Â®Write your article below! Instructions are available [here](https://data1010.github.io/project/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "c80288b5-b900-420d-b252-dc5d7b34f213"
   },
   "source": [
    "# Neural Net Language Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "f4b41a97-2776-448a-8fa9-e8fc81a5364b"
   },
   "source": [
    "Dated from the 1940s to the present, Neural Network has been widely used across numerous fields. In this article, I am going to first elucidate how Neural Networks are applied to language modeling and then introduce some real-world applications of the language models.\n",
    "\n",
    "Let's start with a simple and motivating example:\n",
    "\n",
    "Suppose we have two features with $X_{1}$ = {farmers, cows} , $X_{2}$ = {eat, grow}, and $X_{3}$   = {steak, hay}. The problem here is to find a sensible combination of {$x_{1}$ , $x_{2}$ , $x_{3}$ } where $x_{1} \\in X_{1}, x_{2} \\in X_{2}$ , $ x_{3} \\in X_{3}$ .\n",
    "\n",
    "From our common sense, we know that there are three appropriate combinations:\n",
    "$$\\{farmers, eat, steak\\}$$\n",
    "$$\\{farmers, grow, hay\\}$$\n",
    "$$\\{cows, eat, hay\\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "57dbbcb6-d457-4510-a30a-c5454567b3a1"
   },
   "source": [
    "This is a typical language modeling problem. The goal of **Language modeling (LM)** is to evaluate whether a sentence makes sense in natural language settings by determining a given sequence of words occurring in a sentence using various techniques. There are two primary types of Language models:\n",
    "\n",
    "1. **Statistical language modeling** is a traditional branch of LM that assigns probabilities to the sequences of words and calculates the probabilities of different sentences of which the maximum probability corresponds to the best combination.\n",
    "2. **Neural Language modeling** is a recent trend and has surpassed the statistical language models in their effectiveness. Rather than using the statistical properties, the Neural Language modeling uses neural network architecture to predict the probability of a sequence of words. And the **Feedforward Neural Net Language Model **is the most basic and simplest model in the Neural Language model family.\n",
    "\n",
    "\n",
    "\n",
    "In the example we've talked about above, our goal is to maximize $P(x_{1},x_{2},x_{3})$ . Unlike the assumption of independence in the Naive Bayes model, we assume that each word is based on the previous words in a sequence, eg: $P(x_{2}|x_{1})$ , $P(x_{3}|x_{1},x_{2})$ \n",
    "\n",
    "And thus, the probability of the whole sentence is:\n",
    "$$\\displaystyle P(s) = P(x_{1},x_{2},x_{3}) = P(x_{1})P(x_{2}|x_{1})P(x_{3}|x_{1},x_{2})$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "f25868f1-4d8f-4c4a-a5df-5fc5ea42fb82"
   },
   "source": [
    "In order to maximize $P(x_{1},x_{2},x_{3})$ starting from the same $x_{1}$, we can first find the $x_{2}$ that maximizes $P(x_{2}|x_{1})$ and then find the $x_{3}$  that maximized $P(x_{3}|x_{1},x_{2})$ .  The algorithm is quite simple with each step doing the same thing which is to find the maximum probability of a certain word at the nth position of a sequence given the n-1 previous words.\n",
    "\n",
    "To solve the problem, we introduce **Feedforward Neural Net Language Model **to predict the probability of a certain word at the nth position of a sequence given the n-1 previous words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "562648af-714b-407d-9851-2c641f91bd49"
   },
   "source": [
    "Let's look into the architecture of the Feedforward Neural Net Language Model with the previous example. \n",
    "\n",
    "Suppose we have a corpus $V$ containing the six words given above \n",
    "\n",
    "$$\\displaystyle V =\\{farmers, cows, eat, grow, steak, hay\\} $$\n",
    "\n",
    "Given that the first two words are \"farmers\" and \"eat\", we want to know the probability of the \"steak\" and \"hay\" at the third position of the sequence respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "ac3c8bc3-db49-481e-923e-ffb498f04273"
   },
   "source": [
    "Here, we use the one-hot representation technique, so \"farmers\" is [1,0,0,0,0,0] and \"eat\" is [0,0,1,0,0,0]. \n",
    "\n",
    "Input layer:\n",
    "$$\\displaystyle  v_{i} = C'*x_{i}$$ \n",
    "The mapping C maps any element $x_{i}$  of corpus $V$  to a real vector $v_{i} \\in R^{m}$ . The vector $v_{i}$  represents **the distributed feature vectors **associated with each word in the vocabulary where C is a $|V|*m$ matrix. This process is also called **word embedding** which is to map a sparse vector into a lower-dimensional space by a linear transformation. \n",
    "$\\displaystyle  v = (v_{1},v_{2})$\n",
    "We form the vector $v$ by concatenating the distributed feature vectors of the previous words and, in this case, $v_{1}$ and $v_{2}$. \n",
    "\n",
    "Hidden layer:\n",
    "$$\\displaystyle  y = W*v+b$$\n",
    "$$\\displaystyle h = tanh.(y)$$\\\n",
    "where we take the $v$ as input (output of the previous layer) and get the vector h as output. \".\" means it is a component-wise function. To know more about the tanh function, please read the [hyperbolic function](https://en.wikipedia.org/wiki/Hyperbolic_functions) on Wikipedia.\n",
    " \n",
    "Output layer:\n",
    "$$\\displaystyle z = W_{2}h+b_{2}$$\n",
    "where z is a vector with the length equal to the size of the corpus $|v|$ which is 6 in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "9ecd186f-1a59-4914-bbcb-671c0ec4e4bf"
   },
   "source": [
    "To find the probability of each word at the third position of the sequence of words, we use the softmax function. So the probability that $x_{3}$ is \"steak\" is :\n",
    "\n",
    "$$\\displaystyle P(x_{3} = steak| x_{1} = farmers,x_{2} = eat) = \\frac{e^{z_{5}}}{\\sum_{i}e^{z_{i}}}$$\n",
    "\n",
    "Since the representation of \"steak\" is [0,0,0,0,1,0], \"steak\" is at the fifth position of the z vector.\n",
    "\n",
    "Also, the probability that x_{3} is \"hay\" is:\n",
    "\n",
    "\n",
    "$$\n",
    "\\displaystyle P(x_{3} = hay|x_{1}= farmers, x_{2} = eat)  = \\frac{e^{z_{6}}}{\\sum_{i}{e^{z_{i}}}}\n",
    "$$\n",
    "\n",
    "Since the index of \"hay\" in the z vector is 6, same as the index in the corpus $V$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "2d7585c6-1aab-4699-bc51-0006f9effb75"
   },
   "source": [
    "![](https://firebasestorage.googleapis.com/v0/b/prismia.appspot.com/o/user-images%252Fimage-f773b81d-31a4-47e2-be2d-dd5b6b98a234.png?alt=media&token=983ec9fb-97bc-4c1d-a995-e0a6d27543f0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "38668126-6c1c-4869-b411-427395760758"
   },
   "source": [
    "The training process is similar to the one in the [preclass video](https://www.youtube.com/watch?v=Duh9OLys5gk&list=PLhvJMD5FHvEttUU1FL0kiWDWzZV5-Is12&index=27) with the only difference being that this problem is a multi-class problem while the one in the preclass video is a binary classification problem.\n",
    "\n",
    "However, when we evaluate a statistical language model, we often use a different evaluation index which is **Perplexity.**\n",
    "$$\\displaystyle PP(s) = P(x_{1},...,x_{n})^{-\\frac{1}{n}}$$ \n",
    "By using perplexity, we can transform the maximum problem into a minimum problem so as to perform the stochastic gradient descent algorithm for optimization and the optimal sequence of words will be the one that has the minimum perplexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "4cedf70b-c6a1-4c70-8b2a-0bcf86a4cba8"
   },
   "source": [
    "In the example above, the perplexity of the Neural Net Language Model is:\n",
    "$$\\displaystyle PP(s) = P(x_{1},x_{2},x_{3})^{-\\frac{1}{3}}$$\n",
    " Let's use log function on both sides:\n",
    "$$\\displaystyle log(PP(s)) = -\\frac{1}{3} log(P(x_{1})P(x_{2}|x_{1})P(x_{3}|x_{1},x_{2}))$$\n",
    "So the loss function is:\n",
    "$$\\displaystyle  L  =  -\\frac{1}{3} (log(P(x_{1}))+log(P(x_{2}|x_{1}))+log(P(x_{3}|x_{1},x_{2})))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "df38c591-af4d-460f-8215-caef6039bfaa"
   },
   "source": [
    "The example given above has a very small size of vocabulary. However, if the corpus is of significant size (n), then we will have so many xs that our data will be too sparse. In order to solve this, we introduce the **N-gram** which is a contiguous sequence of n items from a given sample of text. \n",
    "\n",
    "It comes from the idea of the Markov assumption that a future event (in this case, the next word) can be predicted using a relatively short history(in this case, one or two previous words). So:\n",
    "$$\\displaystyle Unigram:  P(s) = P(x_{1}) P(x_{2}) P(x_{3})$$\n",
    "$$\\displaystyle Bigram: P(s) = P(x_{1})P(x_{2}|x_{1})P(x_{3}|x_{2})$$\n",
    "$$\\displaystyle Trigram: P(s) = P(x_{1})  P(x_{2}|x_{1}) P(x_{3}|x_{1},x_{2})$$\n",
    "And if we have a sequence of words with a length of n, k gram will be:\n",
    "$$\\displaystyle  k-gram: P(s) = P(x_{1})P(x_{2}|x_{1})...P(x_{n}|x_{n-k},...,x_{n-1})$$\n",
    "\n",
    "The n-gram we've talked about here is different from the n-gram language model though they have similar ideas. The n-gram language model is a typical statistical language model that uses the statistical properties of a given text. For example, in the n-gram language model, $P(x_{1})$  is equal to the frequency of the word $x_{1}$ in the given corpus and $P(x_{2}|x_{1})$ is the co-occurrence of the word pair {$x_{1}$, $x_{2}$} in the given corpus and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "a4c4438c-9240-4ae6-bc6f-74161c7699de"
   },
   "source": [
    "When we expand the sequence length from 3 to n and we have m words in the corpus $V$,  by using the k-gram, the input of the Feedforward Neural Net Language Model will depend on how many words are before the ith position where the maximum number of the input will be k, the minimum number of the input will be 1.\n",
    "\n",
    "The output of the output layer will be a vector of length m with each entry corresponding to the relative probability of a certain word in the vocabulary $V$.\n",
    "\n",
    "For training, the loss function will be:\n",
    "$$\\displaystyle L = -\\frac{1}{n} \\sum_{i=1}^{n} log P (x_{i}|x_{i-k+1},...,x_{i-1})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "64495f3b-c666-4e65-a64a-a931deba5837"
   },
   "source": [
    "Despite the model itself, the FFNNLM (Feedforward neural net language model) has proposed the idea of word representation as its by-product. Based on FFNNLM, the two word representation models CBOW and Skip-gram, core of the word2vec model, were proposed by Mikolov in 2013.\n",
    "\n",
    "What's more, there have been various improvements in recent decades based on FFNNLM. The first one is the **Recurrent Neural Net Language Model(RNNLM)**. Compared with the structure of FFNNLM, the RNNLM has two differences: one is that it uses the sigmoid function in the hidden layer rather than the tanh function, the other is that in the input layer, the vector $v$ not only contains the distributed feature vector of the n-1 previous words but also incorporates the output of the hidden layer of the previous RNNLM which is to compute the probability of a word in the i-1th position in a sequence of words as the current RNNLM is to compute the probability of the word in the ith position. And thus RNNLM can take advantage of all contexts for prediction while the  FFNNLM only considers the previous k words for prediction.\n",
    "\n",
    "The** Long short-term memory recurrent neural net language model (LSTM-RNNLM)** is a more advanced and complex language model that introduces the memory unit and gate structure to the architecture of RNNLM. And the goal is to solve the problem of the disappearance or explosion of the gradients of parameters for training models to learn long-term dependencies in RNNLM. \n",
    "\n",
    "Last but not least, the **Bidirectional models** including Bi-RNN and Bi-LSTM take advantage of the past and future contexts rather than only the past contexts by preprocessing the input data in both directions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "1142811c-ad03-4b5a-a50a-4ee63b64ca42"
   },
   "source": [
    "The language model is a crucial way for the machine to understand natural languages since each language model type, in one way or another, transforms the qualitative property of the natural language into the quantitative property. And it has been widely used across various industries including tech, finance, healthcare, legal, and transportation.\n",
    "\n",
    "We have encountered many applications of the language models, in one way or another, during our daily life and I've listed several examples below:\n",
    "\n",
    "- **Speech recognition**. Voice assistants like Siri or Alexa and AI agents in customer service are the common application of language models that transform audio information into something quantitative features that machines are able to process.\n",
    "- **Machine Translation**. Translators of big tech companies like Google or Microsoft are the most common usages. We can see that nowadays, the translators have excellent performance that even outperforms human translators with respect to the number of languages one translator can have.\n",
    "- **Sentiment Analysis**. It has been widely used in e-commerce. The main goal of sentiment analysis is to understand the opinions and attitudes expressed in a text. As there are a large number of product reviews on e-commerce platforms like Amazon or eBay, people running businesses on these platforms can use the sentiment analysis technique to analyze customers' attitudes and find ways to improve customer experience and thus increase their profits.\n",
    "- **Information Retrieval. **Search engines like Google are the most common applications as we use them almost every day to search for documents and meta-information like graphs and videos. Benefitting from language models, the search engines have been advanced into a more sophisticated system that has more functions like question answering that can form the answers from the unstructured collection of natural language documents all by the machines themselves given the questions that we've asked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "c6cf5758-6a00-45f5-bf41-44a00453c2a3"
   },
   "source": [
    "To summarize, in this article, I first dive into the detailed architecture of the Feedforward Neural Net Language model by using a simple example, and then briefly introduce the modification and improvement of neural net language models including RNN, LSTM, Bi-RNN and Bi-LSTM, and list some of the applications of language models in our real life. Hopefully, this article will shed light on how neural networks can be applied to natural language processing, and especially language modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "prismiaId": "53345532-9bab-4d66-8dc8-1d941c21c3c7"
   },
   "source": [
    "Reference:\n",
    "\n",
    "[1] Bengio, Y., Ducharme, R., & Vincent, P. (2001). A neural probabilistic language model. InÂ _Advances in Neural Information Processing Systems_Â (pp. 932-938).\n",
    "\n",
    "[2] Lutkevich, B. (2020, March 2). _What is language modeling?_ SearchEnterpriseAI. Retrieved November 28, 2021, from https://searchenterpriseai.techtarget.com/definition/language-modeling. \n",
    "\n",
    "[3] Neubig, G. (2018b). Language Models 3: Neural Networks and Feed-forward Language Models [Slides]. Phontron. http://www.phontron.com/class/mtandseq2seq2018/assets/slides/mt-fall2018.chapter5.pdf \n",
    "\n",
    "[4] Jing, K., & Xu, J. (2019). A survey on neural network language models.Â _arXiv preprint arXiv:1906.03591_.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
