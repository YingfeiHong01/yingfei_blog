{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on Andrew Ng's machine learning course on coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For this experiment, we will identify hand written digits based on multi class logistic regression. The idea is very simple: for each digit, we will train a logistic regression model to predict whether the image is the digit or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat #tool for loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "The data type is mat, so we need the scipy tool to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = loadmat('ex3data1.mat')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['X'].shape, data['y'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in X represents the pixels in a 20 x 20 image. And y represents the digit label for each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid function\n",
    "The sigmoid function can be written as:\n",
    "$$g(z)=\\frac{1}{1+{{e}^{-z}}}$$\n",
    "\n",
    "Given the input matrix X and coefficient $\\theta$, the sigmoid funtion for logistic regression is:\n",
    "$${{h}_{\\theta}}(x)=\\frac{1}{1+{{e}^{-{{\\theta}^{T}}X}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}{[-{{y}^{(i)}}\\log({{h}_{\\theta}}({{x}^{(i)}}))-(1-{{y}^{(i)}})\\log(1-{{h}_{\\theta}}({{x}^{(i)}}))]}+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}{\\theta _{j}^{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, X, y, learningRate,lambda_):\n",
    "    #theta: 1*401 vector \n",
    "    #lambda_: regularization hyperparameter\n",
    "    \n",
    "    # STEP1：transform the X, y, and theta to np.matrix\n",
    "    # your code here  (appro ~ 3 lines)\n",
    "    theta = None\n",
    "    X = None\n",
    "    y = None\n",
    "    \n",
    "    # STEP2：calculate the binary cross entropy loss\n",
    "    # your code here  (appro ~ 2 lines)\n",
    "  \n",
    "    cross_cost = None\n",
    "    \n",
    "    # STEP3：calculate the regularization loss\n",
    "    # your code here  (appro ~ 1 lines)\n",
    "    reg = None\n",
    "   \n",
    "    # STEP4：sum the binary cross entropy loss and regularization loss\n",
    "    # your code here  (appro ~ 1 lines)\n",
    "    whole_cost = None\n",
    "    \n",
    "    return whole_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat until convergence{\n",
    "$$\\theta_{0}:= \\theta_{0} - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}[h_{\\theta}(x^{i}-y^{i})]x_{0}^{i}$$\n",
    "$$\\theta_{j}:= \\theta_{j} - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}[h_{\\theta}(x^{i}-y^{i})]x_{0}^{i} + \\frac{\\lambda}{m}\\theta_{j}$$\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(theta, X, y, learningRate, lambda_):\n",
    "    #theta: 1*401 vector \n",
    "    #lambda_: regularization hyperparameter\n",
    "\n",
    "    \n",
    "    # STEP1：transform the X, y, and theta to np.matrix\n",
    "    # your code here  (appro ~ 3 lines)\n",
    "    theta = None\n",
    "    X = None\n",
    "    y = None\n",
    "    \n",
    "    # STEP2：flatten the theta\n",
    "    # your code here  (appro ~ 1 lines)\n",
    "    parameters = None\n",
    "    \n",
    "    # STEP3：calculate the error\n",
    "    # your code here  (appro ~ 1 lines)    \n",
    "    error = None\n",
    "    \n",
    "    # STEP4：calculate the gradient\n",
    "    # your code here  (appro ~ 1 lines)\n",
    "    grad = None\n",
    "    \n",
    "    # STEP5：no regularization when j = 0\n",
    "    # your code here  (appro ~ 1 lines)\n",
    "    grad[0, 0] = None\n",
    "    \n",
    "    return np.array(grad).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we have 10 possible classes so we need multi-class classification strategy as logistic regression can only deal with two classes. In this experiment, we are going to build 10 different classifiers for 10 different classes and each classifier will decide whether this datapoint belongs to one class or not. \n",
    "\n",
    "In the next cell, we will wrap up the training process in the 'one_vs_all' function and return the 10 thetas for 10 classifiers. Each theta will have 401 elements because we have 401 parameters (including $x_{0}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def one_vs_all(X, y, num_labels, learning_rate, lambda_= 1):\n",
    "    rows = X.shape[0]\n",
    "    params = X.shape[1]\n",
    "    \n",
    "    # k * (n + 1) array for the parameters of each of the k classifiers\n",
    "    all_theta = np.zeros((num_labels, params + 1))\n",
    "    \n",
    "    # insert a column of ones at the beginning for the intercept term\n",
    "    X = np.insert(X, 0, values=np.ones(rows), axis=1)\n",
    "    \n",
    "    # labels are 1-indexed instead of 0-indexed\n",
    "    for i in range(1, num_labels + 1):\n",
    "        theta = np.zeros(params + 1) \n",
    "        y_i = np.array([1 if label == i else 0 for label in y]) #change y_i to 1 or 0 for class i\n",
    "        y_i = np.reshape(y_i, (rows, 1))\n",
    "        \n",
    "        # minimize the objective function\n",
    "        fmin = minimize(fun=cost, x0=theta, args=(X, y_i, learning_rate,lambda_), method='TNC', jac=gradient)\n",
    "        all_theta[i-1,:] = fmin.x\n",
    "    \n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is to show the shape of the parameters.\n",
    "rows = data['X'].shape[0]\n",
    "params = data['X'].shape[1]\n",
    "\n",
    "all_theta = np.zeros((10, params + 1))\n",
    "\n",
    "X = np.insert(data['X'], 0, values=np.ones(rows), axis=1)\n",
    "\n",
    "theta = np.zeros(params + 1)\n",
    "\n",
    "y_0 = np.array([1 if label == 0 else 0 for label in data['y']])\n",
    "y_0 = np.reshape(y_0, (rows, 1))\n",
    "\n",
    "X.shape, y_0.shape, theta.shape, all_theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data['y'])#the label of 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_theta = one_vs_all(data['X'], data['y'], 10, 1, 1)\n",
    "all_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all(X, all_theta):\n",
    "    # TODO：prediction for test data\n",
    "    \n",
    "    # STEP1：\n",
    "    rows = X.shape[0] #number of data points\n",
    "    params = X.shape[1] #number of parameters\n",
    "    num_labels = all_theta.shape[0] #number of labels\n",
    "    \n",
    "    # STEP2：Insert a column of all ones to X\n",
    "    # your code here  (appro ~ 1 lines)\n",
    "    X = None\n",
    "    \n",
    "    # STEP3：Transform X and all_theta to numpy matrix\n",
    "    # your code here  (appro ~ 2 lines)\n",
    "    X = None\n",
    "    all_theta = None\n",
    "    \n",
    "    # STEP4：Calculate the probability of X in all 10 classes\n",
    "    # your code here  (appro ~ 1 lines)\n",
    "    h = sigmoid(None)\n",
    "    \n",
    "    # STEP5：Find the maximum probability for each data point\n",
    "    # your code here  (appro ~ 1 lines)\n",
    "    h_argmax = None\n",
    "    \n",
    "    # STEP6：Add one to get the real label (python is 0-indexed)\n",
    "    h_argmax = h_argmax + 1\n",
    "    \n",
    "    return h_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_all(data['X'], all_theta)\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, data['y'])]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print ('accuracy = {0}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are right, your accuracy should be larger than 90%"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
